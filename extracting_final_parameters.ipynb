{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173615fd",
   "metadata": {},
   "source": [
    "Extracting the final parameter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3d64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b5daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameter values and losses into np.arrays\n",
    "def create_list(file_name, log_path):\n",
    "    if log_path.endswith('auto_tune_iconaml'):\n",
    "        no_params = 24\n",
    "    elif log_path.endswith('auto_tune_icona_baseline') or log_path.endswith('icon-a-ml/run'):\n",
    "        no_params = 19\n",
    "    param_values = np.zeros((1000, no_params))\n",
    "    loss_values = []; net_toa_metric = []; loss_after_toa_values = []\n",
    "    file_path = os.path.join(log_path, file_name)\n",
    "    print_bool = False\n",
    "    eval_count = 0\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if print_bool:\n",
    "                for param_ind in range(no_params):\n",
    "                    # Append certain parameter value\n",
    "                    param_values[eval_count, param_ind] = float(line.split(',')[param_ind])\n",
    "                print_bool = False\n",
    "                eval_count += 1\n",
    "            # Append line after \"Current list of parameters to tune\"\n",
    "            if line.startswith(\"Current list of parameters to tune\"):\n",
    "                print_bool = True\n",
    "            if line.startswith(\"Loss after adding clt_SE_pac:\"):\n",
    "                loss_values.append(float(line.split(':')[1]))\n",
    "            if line.startswith(\"Loss after adding net_toa:\"):\n",
    "                loss_after_toa_values.append(float(line.split(':')[1]))\n",
    "            # TOA value\n",
    "            if line.startswith(\"Metric net_toa:\"):\n",
    "                net_toa_metric.append(float(file.readline()[2:-2]))\n",
    "    return param_values[:len(loss_values), :], np.array(loss_values), np.array(loss_after_toa_values), np.array(net_toa_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a4270",
   "metadata": {},
   "source": [
    "For ICON-A-MLe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f7cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_iconaml = \"/work/bd1083/b309170/published_code/grundner25pnas_iconaml_automatic_tuning/tuning_scripts/auto_tune_iconaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f0beed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.09992098e+01, 7.64343601e+01, 2.34842319e+01, 1.32955979e+00,\n",
       "       3.56392963e-01, 4.40014986e+00, 1.15341387e+01, 2.57516328e+00,\n",
       "       7.58520407e-01, 6.64995145e-01, 1.01689709e+00, 1.98725558e-04,\n",
       "       2.41797118e-04, 3.49049307e-04, 1.98434815e+00, 1.03586510e+01,\n",
       "       9.15158231e-01, 8.30158226e-01, 3.81805561e-01, 8.89074192e-01,\n",
       "       1.90841672e+01, 2.16284838e+02, 2.09014571e+01, 4.20377280e+01])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First year-long tuning. Read parameter values and corresponding losses\n",
    "param_values, loss_values, loss_after_toa_values, net_toa_metric = create_list(\"log.auto_tune.12956787.o\", log_path_iconaml)\n",
    "\n",
    "# Index where loss is minimal.\n",
    "min_ind = np.argmin(loss_values)\n",
    "\n",
    "# Adjust initial guess by adding a weighted difference, motivated by reducing loss_after_toa_values\n",
    "C = loss_after_toa_values[0]/(loss_after_toa_values[0] - loss_after_toa_values[min_ind])\n",
    "\n",
    "# Those are the parameters I call refined_month_240925, however the second parameter was rounded to 76!!\n",
    "param_values[0] + C*(param_values[min_ind] - param_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa77536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great TOA at the expense of the loss\n",
      "Taking the parameter setting with higher loss but better TOA balance\n",
      "Associated loss: 1.773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second year-long tuning. Are the final reported values the ones that minimize the loss in the last script?\n",
    "param_values, loss_values, loss_after_toa_values, net_toa_metric = create_list(\"log.auto_tune.12962670.o\", log_path_iconaml)\n",
    "\n",
    "# Index where loss is minimal\n",
    "min_ind = np.argmin(loss_values)\n",
    "\n",
    "great_toa_inds = np.argwhere((0.5 < net_toa_metric) & (net_toa_metric < 1))\n",
    "\n",
    "# Index for which TOA is great\n",
    "if len(great_toa_inds) >= 1:\n",
    "    print('Great TOA at the expense of the loss')\n",
    "    \n",
    "    # Extract index with best loss out of the ones with great TOA\n",
    "    best_loss = np.squeeze(great_toa_inds[np.argmin(loss_values[great_toa_inds])])\n",
    "\n",
    "    # Prefer parameters with a perfect TOA even if it slightly hurts the loss\n",
    "    if (loss_values[best_loss] - loss_values[min_ind])/loss_values[min_ind] < 0.25:\n",
    "        print('Taking the parameter setting with higher loss but better TOA balance')\n",
    "        final_parameters = param_values[best_loss]\n",
    "    else:\n",
    "        final_parameters = param_values[min_ind]\n",
    "else:\n",
    "    final_parameters = param_values[min_ind]\n",
    "    best_loss = min_ind\n",
    "\n",
    "# Final set of parameters\n",
    "final_parameters\n",
    "\n",
    "# Set of parameters I've used for the simulation in the paper. See here: https://github.com/EyringMLClimateGroup/grundner25pnas_iconaml_automatic_tuning/blob/main/simulation_scripts_and_evaluation/auto_tuned_iconaml_full/exp.ag_atm_amip_r2b5_cvtfall_entrmid_05_cov15_based_on_12962670_20yrs_240927.run\n",
    "paper_parameters = [40.999209779803124, 79.8, 23.484231887639147, 1.3295597852519498, 0.35639296308199575, 4.400149863408263, 11.534138730085267, 2.5751632819728894, 0.7585204066979654, 0.6649951451999998, 1.0168970920619893,\\\n",
    "                    0.00019872555789959162, 0.00024179711798483726, 0.0003490493071823422, 1.9843481475548161, 10.358651006058587, 0.9151582312253206, 0.8301582264050795, 0.38180556104206254,\\\n",
    "                    0.8890741916120648, 19.084167204527887, 216.2848380476869, 20.901457054208564, 42.03772802962055]\n",
    "\n",
    "print('Associated loss: %.3f'%loss_values[best_loss])\n",
    "\n",
    "# They are the same\n",
    "paper_parameters - final_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e88e1f",
   "metadata": {},
   "source": [
    "For ICON-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7b8c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_icona = \"/work/bd1083/b309170/published_code/grundner25pnas_iconaml_automatic_tuning/tuning_scripts/auto_tune_icona_baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7eba29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00013844e+00, 5.95913147e-01, 7.01698320e-01, 2.16944655e+00,\n",
       "       2.48621838e-01, 1.05889486e+00, 2.18095141e-04, 2.05913392e-04,\n",
       "       3.98469210e-04, 2.18646378e+00, 1.66970491e+01, 8.45743113e-01,\n",
       "       8.64875152e-01, 3.89880022e-01, 8.32452621e-01, 2.09874253e+01,\n",
       "       1.77609917e+02, 2.02055062e+01, 8.19499853e+01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First year-long tuning. Read parameter values and corresponding losses\n",
    "param_values, loss_values, loss_after_toa_values, net_toa_metric = create_list(\"log.auto_tune_baseline.13852199.o\", log_path_icona)\n",
    "\n",
    "# Index where loss is minimal.\n",
    "min_ind = np.argmin(loss_values)\n",
    "\n",
    "# Adjust initial guess by adding a weighted difference, motivated by reducing loss_after_toa_values\n",
    "C = loss_after_toa_values[0]/(loss_after_toa_values[0] - loss_after_toa_values[min_ind])\n",
    "\n",
    "# With the same logic as above\n",
    "param_values[0] + C*(param_values[min_ind] - param_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe6e9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 3.777\n",
      "Final TOA loss: 0.805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.90000000e-01, 5.95913147e-01, 7.01698320e-01, 2.16944655e+00,\n",
       "       2.48621838e-01, 1.05889486e+00, 2.18095141e-04, 2.05913392e-04,\n",
       "       3.98469210e-04, 2.18646378e+00, 1.66970491e+01, 8.88030268e-01,\n",
       "       8.64875152e-01, 3.89880022e-01, 8.32452621e-01, 2.09874253e+01,\n",
       "       1.77609917e+02, 2.02055062e+01, 8.19499853e+01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second year-long tuning. Are the final reported values the ones that minimize the loss in the last script?\n",
    "param_values, loss_values, loss_after_toa_values, net_toa_metric = create_list(\"log.auto_tune_baseline.17820065.o\", log_path_icona)\n",
    "\n",
    "# Index where loss is minimal\n",
    "min_ind = np.argmin(loss_values)\n",
    "\n",
    "great_toa_inds = np.argwhere((0.5 < net_toa_metric) & (net_toa_metric < 1))\n",
    "\n",
    "# Index for which TOA is great\n",
    "if len(great_toa_inds) >= 1:\n",
    "    print('Great TOA at the expense of the loss')\n",
    "    \n",
    "    # Extract index with best loss out of the ones with great TOA\n",
    "    best_loss = np.squeeze(great_toa_inds[np.argmin(loss_values[great_toa_inds])])\n",
    "\n",
    "    # Prefer parameters with a perfect TOA even if it slightly hurts the loss\n",
    "    if (loss_values[best_loss] - loss_values[min_ind])/loss_values[min_ind] < 0.25:\n",
    "        print('Taking the parameter setting with higher loss but better TOA balance')\n",
    "        final_parameters = param_values[best_loss]\n",
    "    else:\n",
    "        final_parameters = param_values[min_ind]\n",
    "else:\n",
    "    final_parameters = param_values[min_ind]\n",
    "    best_loss = min_ind\n",
    "\n",
    "# Final loss\n",
    "print('Final loss: %.3f'%loss_values[best_loss])\n",
    "print('Final TOA loss: %.3f'%loss_after_toa_values[best_loss])\n",
    "\n",
    "# Final set of parameters\n",
    "final_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2611df85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great TOA at the expense of the loss\n",
      "Taking the parameter setting with higher loss but better TOA balance\n",
      "Loss after tweaking: 3.521\n",
      "TOA loss after tweaking: 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.68000000e-01, 6.65000000e-01, 7.01698320e-01, 2.16944655e+00,\n",
       "       2.61052930e-01, 1.05889486e+00, 2.18095141e-04, 2.05913392e-04,\n",
       "       3.98469210e-04, 2.18646378e+00, 1.66970491e+01, 8.45743113e-01,\n",
       "       8.64875152e-01, 3.89880022e-01, 8.32452621e-01, 2.09874253e+01,\n",
       "       1.77609917e+02, 2.02055062e+01, 8.19499853e+01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_values, loss_values, loss_after_toa_values, net_toa_metric = create_list(\"log.auto_tune_baseline.13887792.o\", log_path_icona)\n",
    "\n",
    "# Index where loss is minimal\n",
    "min_ind = np.argmin(loss_values)\n",
    "\n",
    "great_toa_inds = np.argwhere((0.5 < net_toa_metric) & (net_toa_metric < 1))\n",
    "\n",
    "# Index for which TOA is great\n",
    "if len(great_toa_inds) >= 1:\n",
    "    print('Great TOA at the expense of the loss')\n",
    "    \n",
    "    # Extract index with best loss out of the ones with great TOA\n",
    "    best_loss = np.squeeze(great_toa_inds[np.argmin(loss_values[great_toa_inds])])\n",
    "\n",
    "    # Prefer parameters with a perfect TOA even if it slightly hurts the loss\n",
    "    if (loss_values[best_loss] - loss_values[min_ind])/loss_values[min_ind] < 0.25:\n",
    "        print('Taking the parameter setting with higher loss but better TOA balance')\n",
    "        final_parameters = param_values[best_loss]\n",
    "    else:\n",
    "        final_parameters = param_values[min_ind]\n",
    "else:\n",
    "    final_parameters = param_values[min_ind]\n",
    "    best_loss = min_ind\n",
    "\n",
    "print('Loss after tweaking: %.3f'%loss_values[best_loss])\n",
    "print('TOA loss after tweaking: %.3f'%loss_after_toa_values[best_loss])\n",
    "\n",
    "# Final parameters\n",
    "param_values[min_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8902527f",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b072164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss after re-running year-long simulations after the tweak\n",
    "# param_values, loss_values, loss_after_toa_values, net_toa_metric = create_list(\"log.auto_tune_baseline.13887792.o\", log_path_icona)\n",
    "\n",
    "# min_ind = np.argmin(loss_values)\n",
    "# great_toa_inds = np.argwhere((0.5 < net_toa_metric) & (net_toa_metric < 1))\n",
    "\n",
    "# # Index for which TOA is great\n",
    "# if len(great_toa_inds) >= 1:\n",
    "#     print('Great TOA at the expense of the loss')\n",
    "    \n",
    "#     # Extract index with best loss out of the ones with great TOA\n",
    "#     best_loss = np.squeeze(great_toa_inds[np.argmin(loss_values[great_toa_inds])])\n",
    "\n",
    "#     # Prefer parameters with a perfect TOA even if it slightly hurts the loss\n",
    "#     if (loss_values[best_loss] - loss_values[min_ind])/loss_values[min_ind] < 0.25:\n",
    "#         print('Taking the parameter setting with higher loss but better TOA balance')\n",
    "#         final_parameters = param_values[best_loss]\n",
    "#     else:\n",
    "#         final_parameters = param_values[min_ind]\n",
    "# else:\n",
    "#     final_parameters = param_values[min_ind]\n",
    "\n",
    "# print('Loss after tweaking and rerunning: %.3f'%loss_values[best_loss])\n",
    "# print('TOA loss after tweaking and rerunning: %.3f'%loss_after_toa_values[best_loss])\n",
    "\n",
    "# paper_parameters = np.array([9.68000000e-01, 6.98250000e-01, 7.01698320e-01, 2.16944655e+00,\n",
    "#        2.48621838e-01, 1.05889486e+00, 2.18095141e-04, 2.05913392e-04,\n",
    "#        3.98469210e-04, 2.18646378e+00, 1.66970491e+01, 8.45743113e-01,\n",
    "#        8.64875152e-01, 3.89880022e-01, 8.32452621e-01, 2.09874253e+01,\n",
    "#        1.77609917e+02, 2.02055062e+01, 8.19499853e+01])\n",
    "\n",
    "# # Final parameters\n",
    "# final_parameters\n",
    "\n",
    "# final_parameters - paper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "27adbd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust initial guess by adding a weighted difference, motivated by reducing loss_after_toa_values\n",
    "# C1 = loss_after_toa_values[0]/(loss_after_toa_values[0] - loss_after_toa_values[1])\n",
    "# C2 = loss_after_toa_values[0]/(loss_after_toa_values[0] - loss_after_toa_values[2])\n",
    "\n",
    "# alpha1 = 0.63 # Not well motivated\n",
    "# alpha2 = 1-alpha1\n",
    "\n",
    "# final_parameters = param_values[0] + C1*(param_values[1] - param_values[0])*alpha1 + C2*(param_values[2] - param_values[0])*alpha2\n",
    "# final_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "af03d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set of parameters I've used for the simulation in the paper. See here: https://github.com/EyringMLClimateGroup/grundner25pnas_iconaml_automatic_tuning/blob/main/simulation_scripts_and_evaluation/auto_tuned_baseline/exp.ag_atm_amip_r2b5_auto_tuned_baseline_20yrs.run\n",
    "# paper_parameters = [0.968, 0.69825, 0.7016983204535419, 2.169446548166456, 0.24862183830475587, 1.0588948615278229, 0.00021809514111887957, 0.000205913392405527, 0.00039846920997366024,\\\n",
    "#                     2.186463783358046, 16.697049053748103, 0.8457431126829968, 0.8648751522165565, 0.3898800218636592, 0.8324526207821763, 20.98742526609769, 177.60991737322595,\\\n",
    "#                     20.20550621794748, 81.94998525907712]\n",
    "\n",
    "# final_parameters - paper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d076fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust initial guess by adding a weighted difference, motivated by reducing loss_after_toa_values\n",
    "# C1 = loss_after_toa_values[0]/(loss_after_toa_values[0] - loss_after_toa_values[1])\n",
    "# C2 = loss_after_toa_values[0]/(loss_after_toa_values[0] - loss_after_toa_values[2])\n",
    "\n",
    "# # alpha = 1/2 is well motivated\n",
    "# alpha1 = 0.5\n",
    "# alpha2 = 0.5\n",
    "\n",
    "# final_parameters = param_values[0] + C1*(param_values[1] - param_values[0])*alpha1 + C2*(param_values[2] - param_values[0])*alpha2\n",
    "# final_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "dd5dd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Results of intermediary strategies:\n",
    "# log_path_icona = \"/work/bd1179/b309170/icon-ml_models/icon-a-ml/run\"\n",
    "# # 17820065 (year_refined_5) \n",
    "# param_values, loss_values, loss_after_toa_values, net_toa_metric = create_list(\"log.auto_tune_baseline.17820065.o\", log_path_icona)\n",
    "# # Best parameter setting\n",
    "# min_ind = np.argmin(loss_values)\n",
    "# # Parameter that has actually changed\n",
    "# diff_par = np.argmax(param_values[min_ind] - param_values[0])\n",
    "# print('Verdict of year_refined_5:')\n",
    "# print('Changing the %dth parameter from %.3f to %.3f helps most.'%(diff_par+1, param_values[0,diff_par], param_values[min_ind,diff_par]))\n",
    "\n",
    "# print(loss_values[min_ind])\n",
    "\n",
    "# # 17821143 (year_refined_6) \n",
    "# param_values, loss_values, loss_after_toa_values, net_toa_metric = create_list(\"log.auto_tune_baseline.17821143.o\", log_path_icona)\n",
    "# # Best parameter setting\n",
    "# min_ind = np.argmin(loss_values)\n",
    "# # Parameter that has actually changed\n",
    "# diff_par = np.argmax(param_values[min_ind] - param_values[0])\n",
    "# print('Verdict of year_refined_6:')\n",
    "# print('Changing the %dth parameter from %.3f to %.3f helps most.'%(diff_par+1, param_values[0,diff_par], param_values[min_ind,diff_par]))\n",
    "\n",
    "# print(loss_values[min_ind])\n",
    "\n",
    "# # 17821894 (year_refined_7) \n",
    "# param_values, loss_values, loss_after_toa_values, net_toa_metric = create_list(\"log.auto_tune_baseline.17821894.o\", log_path_icona)\n",
    "# # Best parameter setting\n",
    "# min_ind = np.argmin(loss_values)\n",
    "# # Parameter that has actually changed\n",
    "# diff_par = np.argmax(param_values[min_ind] - param_values[0])\n",
    "# print('Verdict of year_refined_7:')\n",
    "# print('Changing the %dth parameter from %.3f to %.3f helps most.'%(diff_par+1, param_values[0,diff_par], param_values[min_ind,diff_par]))\n",
    "\n",
    "# print(loss_values[min_ind])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds_up_to_date",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
